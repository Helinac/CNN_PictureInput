{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "use_ramdon_split = False\n",
    "use_dataparallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Python path: ['C:\\\\Users\\\\zhaox\\\\Documents\\\\Python\\\\Stock_CNN-main\\\\notebooks', 'C:\\\\Users\\\\zhaox\\\\.conda\\\\envs\\\\top\\\\python39.zip', 'C:\\\\Users\\\\zhaox\\\\.conda\\\\envs\\\\top\\\\DLLs', 'C:\\\\Users\\\\zhaox\\\\.conda\\\\envs\\\\top\\\\lib', 'C:\\\\Users\\\\zhaox\\\\.conda\\\\envs\\\\top', '', 'C:\\\\Users\\\\zhaox\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages', 'C:\\\\Users\\\\zhaox\\\\.conda\\\\envs\\\\top\\\\lib\\\\site-packages', 'C:\\\\Users\\\\zhaox\\\\.conda\\\\envs\\\\top\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\zhaox\\\\.conda\\\\envs\\\\top\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\zhaox\\\\.conda\\\\envs\\\\top\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\zhaox\\\\.conda\\\\envs\\\\top\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\zhaox\\\\.ipython']\n",
      "Successfully imported gpu_tools\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Python path:\", sys.path)\n",
    "\n",
    "utils_dir = r'C:\\Users\\zhaox\\Documents\\Python\\Stock_CNN-main\\utils'\n",
    "\n",
    "if utils_dir not in sys.path:\n",
    "    sys.path.append(utils_dir)\n",
    "\n",
    "try:\n",
    "    import gpu_tools\n",
    "    print(\"Successfully imported gpu_tools\")\n",
    "except Exception as e:\n",
    "    print(\"Error importing gpu_tools:\", e)\n",
    "\n",
    "    print(\"\\nContents of the utils directory:\")\n",
    "    for item in os.listdir(utils_dir):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "\n",
    "import os\n",
    "import re \n",
    "\n",
    "def query_gpu(qargs=[]):\n",
    "    qargs =['index','gpu_name', 'memory.free']+ qargs\n",
    "    cmd = 'nvidia-smi --query-gpu={} --format=csv,noheader'.format(','.join(qargs))\n",
    "    results = os.popen(cmd).readlines()\n",
    "    return results\n",
    "\n",
    "def select_gpu(results, thres=4096):\n",
    "    avali = []\n",
    "    try:\n",
    "        for i, line in enumerate(results):\n",
    "            if int(re.findall('(.*), (.*?) MiB', line)[0][-1]) > thres:\n",
    "                avali.append(i)\n",
    "        return avali\n",
    "    except:\n",
    "        return ''\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join([ str(obj) for obj in select_gpu(query_gpu())])\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "IMAGE_WIDTH = {5: 15, 20: 60, 60: 180}\n",
    "IMAGE_HEIGHT = {5: 32, 20: 64, 60: 96}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n",
    "\n",
    "here we choose 1993-2001 data as our training(include validation) data, the remaining will be used in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6576, 48)\n",
      "(2740, 7)\n",
      "          Date       Open       High        Low      Close     Volume  \\\n",
      "0   2009-02-02   2.687241   2.774705   2.681209   2.759926  558247200   \n",
      "1   2009-02-03   2.772292   2.816325   2.722830   2.804261  599309200   \n",
      "2   2009-02-04   2.811500   2.902884   2.807880   2.821452  808421600   \n",
      "3   2009-02-05   2.797927   2.933043   2.793404   2.909217  749246400   \n",
      "4   2009-02-06   2.926107   3.015984   2.925504   3.007539  687209600   \n",
      "..         ...        ...        ...        ...        ...        ...   \n",
      "15  2019-12-12  64.973414  66.133220  64.861803  65.866318  137310400   \n",
      "16  2019-12-13  65.866303  66.798029  65.737705  66.761635  133587600   \n",
      "17  2019-12-16  67.210548  68.130146  67.205698  67.904488  128186000   \n",
      "18  2019-12-17  67.834111  68.367909  67.647276  68.037926  114158400   \n",
      "19  2019-12-18  67.889916  68.399456  67.724925  67.875359  116028400   \n",
      "\n",
      "    20 Day MA  \n",
      "0    2.678585  \n",
      "1    2.676172  \n",
      "2    2.676971  \n",
      "3    2.685190  \n",
      "4    2.695776  \n",
      "..        ...  \n",
      "15  64.466179  \n",
      "16  64.617947  \n",
      "17  64.789007  \n",
      "18  64.950482  \n",
      "19  65.113655  \n",
      "\n",
      "[2740 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the number of segments (you can adjust this based on how many segments you have)\n",
    "num_segments = 137  # Adjust this based on the number of segments you have\n",
    "\n",
    "# Define the target 2D shape (e.g., 48 x 48)\n",
    "desired_height = 48\n",
    "desired_width = 48\n",
    "desired_size = desired_height * desired_width  # 2304 elements in total\n",
    "\n",
    "# Create empty lists to store the images and labels\n",
    "images = []\n",
    "label_df = []\n",
    "\n",
    "# Iterate over each segment (assuming files are named like AAPL_segment_1.dat, AAPL_segment_2.dat, etc.)\n",
    "for segment in range(1, num_segments + 1):\n",
    "    # Load image data from binary .dat file\n",
    "    image_path = f\"AAPL_segment_{segment}.dat\"\n",
    "    \n",
    "    # Read the image data as 1D array\n",
    "    image_data = np.memmap(image_path, dtype=np.uint8, mode='r')\n",
    "    \n",
    "    # Trim or pad the data as needed to fit the desired shape\n",
    "    if image_data.size > desired_size:\n",
    "        # Trim the data if it's larger than the desired size\n",
    "        final_image_data = image_data[:desired_size]\n",
    "    elif image_data.size < desired_size:\n",
    "        # Pad the data with zeros if it's smaller than the desired size\n",
    "        padding_needed = desired_size - image_data.size\n",
    "        final_image_data = np.pad(image_data, (0, padding_needed), mode='constant')\n",
    "    else:\n",
    "        final_image_data = image_data  # No padding or trimming needed\n",
    "\n",
    "    # Reshape the final data into a 48x48 2D array\n",
    "    reshaped_image = final_image_data.reshape((desired_height, desired_width))\n",
    "    \n",
    "    # Append each segment's 2D image data to the list\n",
    "    images.append(reshaped_image)\n",
    "    \n",
    "    # Load label data from feather file\n",
    "    label_path = f\"AAPL_segment_{segment}.feather\"\n",
    "    label_data = pd.read_feather(label_path)\n",
    "    label_df.append(label_data)  # Append each segment's label data\n",
    "\n",
    "\n",
    "    \n",
    "images = np.concatenate(images)\n",
    "label_df = pd.concat(label_df)\n",
    "\n",
    "print(images.shape)\n",
    "print(label_df.shape)\n",
    "print(label_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img, label):\n",
    "        self.img = torch.Tensor(img.copy())\n",
    "        self.label = torch.Tensor(label)\n",
    "        self.len = len(img)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.img[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split method (not random split is recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Ret_20d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18144/1006984639.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtrain_val_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msplit_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msplit_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRet_20d\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msplit_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mval_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRet_20d\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\top\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Ret_20d'"
     ]
    }
   ],
   "source": [
    "if not use_ramdon_split:\n",
    "    train_val_ratio = 0.7\n",
    "    split_idx = int(images.shape[0] * 0.7)\n",
    "    train_dataset = MyDataset(images[:split_idx], (label_df.Ret_20d > 0).values[:split_idx])\n",
    "    val_dataset = MyDataset(images[split_idx:], (label_df.Ret_20d > 0).values[split_idx:])\n",
    "else:\n",
    "    dataset = MyDataset(images, (label_df.Ret_20d > 0).values)\n",
    "    train_val_ratio = 0.7\n",
    "    train_dataset, val_dataset = random_split(dataset, \\\n",
    "        [int(dataset.len*train_val_ratio), dataset.len-int(dataset.len*train_val_ratio)], \\\n",
    "        generator=torch.Generator().manual_seed(42))\n",
    "    del dataset\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import baseline\n",
    "\n",
    "device = 'cuda' if use_gpu else 'cpu'\n",
    "export_onnx = True\n",
    "net = baseline.Net().to(device)\n",
    "net.apply(init_weights)\n",
    "\n",
    "if export_onnx:\n",
    "    import torch.onnx\n",
    "    x = torch.randn([1,1,64,60]).to(device)\n",
    "    torch.onnx.export(net,               # model being run\n",
    "                      x,                         # model input (or a tuple for multiple inputs)\n",
    "                      \"../cnn_baseline.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                      export_params=False,        # store the trained parameter weights inside the model file\n",
    "                      opset_version=10,          # the ONNX version to export the model to\n",
    "                      do_constant_folding=False,  # whether to execute constant folding for optimization\n",
    "                      input_names = ['input_images'],   # the model's input names\n",
    "                      output_names = ['output_prob'], # the model's output names\n",
    "                      dynamic_axes={'input_images' : {0 : 'batch_size'},    # variable length axes\n",
    "                                     'output_prob' : {0 : 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.0.weight : torch.Size([64, 1, 5, 3])\n",
      "layer1.0.bias : torch.Size([64])\n",
      "layer1.1.weight : torch.Size([64])\n",
      "layer1.1.bias : torch.Size([64])\n",
      "layer2.0.weight : torch.Size([128, 64, 5, 3])\n",
      "layer2.0.bias : torch.Size([128])\n",
      "layer2.1.weight : torch.Size([128])\n",
      "layer2.1.bias : torch.Size([128])\n",
      "layer3.0.weight : torch.Size([256, 128, 5, 3])\n",
      "layer3.0.bias : torch.Size([256])\n",
      "layer3.1.weight : torch.Size([256])\n",
      "layer3.1.bias : torch.Size([256])\n",
      "fc1.1.weight : torch.Size([2, 46080])\n",
      "fc1.1.bias : torch.Size([2])\n",
      "total_parameters : 708866\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for name, parameters in net.named_parameters():\n",
    "    print(name, ':', parameters.size())\n",
    "    count += parameters.numel()\n",
    "print('total_parameters : {}'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'models.baseline.Net'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "FLOPs = 36.21961728G\n",
      "Params = 0.708866M\n"
     ]
    }
   ],
   "source": [
    "from thop import profile as thop_profile\n",
    "\n",
    "flops, params = thop_profile(net, inputs=(next(iter(train_dataloader))[0].to(device),))\n",
    "print('FLOPs = ' + str(flops/1000**3) + 'G')\n",
    "print('Params = ' + str(params/1000**2) + 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference        12.02%       1.426ms        66.87%       7.931ms       7.931ms       0.000us         0.00%       9.862ms       9.862ms             1  \n",
      "                                           aten::conv2d         0.26%      31.000us        24.60%       2.917ms     972.333us       0.000us         0.00%       7.306ms       2.435ms             3  \n",
      "                                      aten::convolution         0.36%      43.000us        24.33%       2.886ms     962.000us       0.000us         0.00%       7.306ms       2.435ms             3  \n",
      "                                     aten::_convolution         0.94%     112.000us        23.97%       2.843ms     947.667us       0.000us         0.00%       7.306ms       2.435ms             3  \n",
      "                                aten::cudnn_convolution        17.00%       2.016ms        21.09%       2.501ms     833.667us       6.755ms        68.50%       6.755ms       2.252ms             3  \n",
      "                  volta_scudnn_128x128_relu_small_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us       6.494ms        65.85%       6.494ms       3.247ms             2  \n",
      "                                       aten::batch_norm         0.24%      29.000us        12.36%       1.466ms     488.667us       0.000us         0.00%       1.077ms     359.000us             3  \n",
      "                           aten::_batch_norm_impl_index         0.34%      40.000us        12.12%       1.437ms     479.000us       0.000us         0.00%       1.077ms     359.000us             3  \n",
      "                                 aten::cudnn_batch_norm         2.61%     309.000us        11.78%       1.397ms     465.667us       1.077ms        10.92%       1.077ms     359.000us             3  \n",
      "                                       aten::max_pool2d         0.29%      34.000us         8.54%       1.013ms     337.667us       0.000us         0.00%     656.000us     218.667us             3  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 11.860ms\n",
      "Self CUDA time total: 9.862ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "inputs = next(iter(train_dataloader))[0].to(device)\n",
    "\n",
    "with profile(activities=[\n",
    "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        net(inputs)\n",
    "\n",
    "prof.export_chrome_trace(\"../trace.json\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, net, loss_fn, optimizer):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    current = 0\n",
    "    net.train()\n",
    "    \n",
    "    with tqdm(dataloader) as t:\n",
    "        for batch, (X, y) in enumerate(t):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = net(X)\n",
    "            loss = loss_fn(y_pred, y.long())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = (len(X) * loss.item() + running_loss * current) / (len(X) + current)\n",
    "            current += len(X)\n",
    "            t.set_postfix({'running_loss':running_loss})\n",
    "    \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(dataloader, net, loss_fn):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    current = 0\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(dataloader) as t:\n",
    "            for batch, (X, y) in enumerate(t):\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                y_pred = net(X)\n",
    "                loss = loss_fn(y_pred, y.long())\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                running_loss = (len(X) * running_loss + loss.item() * current) / (len(X) + current)\n",
    "                current += len(X)\n",
    "            \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = torch.load('/home/clidg/proj_2/pt/baseline_epoch_10_train_0.6865865240322523_eval_0.686580_.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu and use_dataparallel and 'DataParallel' not in str(type(net)):\n",
    "    net = net.to(device)\n",
    "    net = nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-5)\n",
    "\n",
    "start_epoch = 0\n",
    "min_val_loss = 1e9\n",
    "last_min_ind = -1\n",
    "early_stopping_epoch = 5\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 551/4337 [00:14<02:33, 24.60it/s, running_loss=0.901]"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now().strftime('%Y%m%d_%H:%M:%S')\n",
    "os.mkdir('../pt'+os.sep+start_time)\n",
    "epochs = 100\n",
    "for t in range(start_epoch, epochs):\n",
    "    print(f\"Epoch {t}\\n-------------------------------\")\n",
    "    time.sleep(0.2)\n",
    "    train_loss = train_loop(train_dataloader, net, loss_fn, optimizer)\n",
    "    val_loss = val_loop(val_dataloader, net, loss_fn)\n",
    "    tb.add_histogram(\"train_loss\", train_loss, t)\n",
    "    torch.save(net, '../pt'+os.sep+start_time+os.sep+'baseline_epoch_{}_train_{:5f}_val_{:5f}.pt'.format(t, train_loss, val_loss)) \n",
    "    if val_loss < min_val_loss:\n",
    "        last_min_ind = t\n",
    "        min_val_loss = val_loss\n",
    "    elif t - last_min_ind >= early_stopping_epoch:\n",
    "        break\n",
    "\n",
    "print('Done!')\n",
    "print('Best epoch: {}, val_loss: {}'.format(last_min_ind, min_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
