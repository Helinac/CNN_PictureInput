{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be85f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "use_ramdon_split = False\n",
    "use_dataparallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8afcb131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "\n",
    "import os\n",
    "import re \n",
    "\n",
    "def query_gpu(qargs=[]):\n",
    "    qargs =['index','gpu_name', 'memory.free']+ qargs\n",
    "    cmd = 'nvidia-smi --query-gpu={} --format=csv,noheader'.format(','.join(qargs))\n",
    "    results = os.popen(cmd).readlines()\n",
    "    return results\n",
    "\n",
    "def select_gpu(results, thres=4096):\n",
    "    avali = []\n",
    "    try:\n",
    "        for i, line in enumerate(results):\n",
    "            if int(re.findall('(.*), (.*?) MiB', line)[0][-1]) > thres:\n",
    "                avali.append(i)\n",
    "        return avali\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join([ str(obj) for obj in select_gpu(query_gpu())])\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "IMAGE_WIDTH = {5: 15, 20: 60, 60: 180}\n",
    "IMAGE_HEIGHT = {5: 32, 20: 64, 60: 96}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f403f87",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20215522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6576, 48)\n",
      "(2740, 7)\n",
      "          Date       Open       High        Low      Close     Volume  \\\n",
      "0   2009-02-02   2.687241   2.774705   2.681209   2.759926  558247200   \n",
      "1   2009-02-03   2.772292   2.816325   2.722830   2.804261  599309200   \n",
      "2   2009-02-04   2.811500   2.902884   2.807880   2.821452  808421600   \n",
      "3   2009-02-05   2.797927   2.933043   2.793404   2.909217  749246400   \n",
      "4   2009-02-06   2.926107   3.015984   2.925504   3.007539  687209600   \n",
      "..         ...        ...        ...        ...        ...        ...   \n",
      "15  2019-12-12  64.973414  66.133220  64.861803  65.866318  137310400   \n",
      "16  2019-12-13  65.866303  66.798029  65.737705  66.761635  133587600   \n",
      "17  2019-12-16  67.210548  68.130146  67.205698  67.904488  128186000   \n",
      "18  2019-12-17  67.834111  68.367909  67.647276  68.037926  114158400   \n",
      "19  2019-12-18  67.889916  68.399456  67.724925  67.875359  116028400   \n",
      "\n",
      "    20 Day MA  \n",
      "0    2.678585  \n",
      "1    2.676172  \n",
      "2    2.676971  \n",
      "3    2.685190  \n",
      "4    2.695776  \n",
      "..        ...  \n",
      "15  64.466179  \n",
      "16  64.617947  \n",
      "17  64.789007  \n",
      "18  64.950482  \n",
      "19  65.113655  \n",
      "\n",
      "[2740 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the number of segments (you can adjust this based on how many segments you have)\n",
    "num_segments = 137  # Adjust this based on the number of segments you have\n",
    "\n",
    "# Define the target 2D shape (e.g., 48 x 48)\n",
    "desired_height = 48\n",
    "desired_width = 48\n",
    "desired_size = desired_height * desired_width  # 2304 elements in total\n",
    "\n",
    "# Create empty lists to store the images and labels\n",
    "images = []\n",
    "label_df = []\n",
    "\n",
    "# Iterate over each segment (assuming files are named like AAPL_segment_1.dat, AAPL_segment_2.dat, etc.)\n",
    "for segment in range(1, num_segments + 1):\n",
    "    # Load image data from binary .dat file\n",
    "    image_path = f\"AAPL_segment_{segment}.dat\"\n",
    "    \n",
    "    # Read the image data as 1D array\n",
    "    image_data = np.memmap(image_path, dtype=np.uint8, mode='r')\n",
    "    \n",
    "    # Trim or pad the data as needed to fit the desired shape\n",
    "    if image_data.size > desired_size:\n",
    "        # Trim the data if it's larger than the desired size\n",
    "        final_image_data = image_data[:desired_size]\n",
    "    elif image_data.size < desired_size:\n",
    "        # Pad the data with zeros if it's smaller than the desired size\n",
    "        padding_needed = desired_size - image_data.size\n",
    "        final_image_data = np.pad(image_data, (0, padding_needed), mode='constant')\n",
    "    else:\n",
    "        final_image_data = image_data  # No padding or trimming needed\n",
    "\n",
    "    # Reshape the final data into a 48x48 2D array\n",
    "    reshaped_image = final_image_data.reshape((desired_height, desired_width))\n",
    "    \n",
    "    # Append each segment's 2D image data to the list\n",
    "    images.append(reshaped_image)\n",
    "    \n",
    "    # Load label data from feather file\n",
    "    label_path = f\"AAPL_segment_{segment}.feather\"\n",
    "    label_data = pd.read_feather(label_path)\n",
    "    label_df.append(label_data)  # Append each segment's label data\n",
    "\n",
    "\n",
    "    \n",
    "images = np.concatenate(images)\n",
    "label_df = pd.concat(label_df)\n",
    "\n",
    "print(images.shape)\n",
    "print(label_df.shape)\n",
    "print(label_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f186211",
   "metadata": {},
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85766fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img, label):\n",
    "        self.img = torch.Tensor(img.copy())\n",
    "        self.label = torch.Tensor(label)\n",
    "        self.len = len(img)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.img[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c80a62",
   "metadata": {},
   "source": [
    "##### Split method (not random split is recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "363405a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 20-day return (Ret_20d) based on the 'Close' price\n",
    "label_df['Ret_20d'] = (label_df['Close'].shift(-20) - label_df['Close']) / label_df['Close']\n",
    "\n",
    "# Now use Ret_20d for the dataset\n",
    "if not use_ramdon_split:\n",
    "    train_val_ratio = 0.7\n",
    "    split_idx = int(images.shape[0] * 0.7)\n",
    "    train_dataset = MyDataset(images[:split_idx], (label_df.Ret_20d > 0).values[:split_idx])\n",
    "    val_dataset = MyDataset(images[split_idx:], (label_df.Ret_20d > 0).values[split_idx:])\n",
    "else:\n",
    "    dataset = MyDataset(images, (label_df.Ret_20d > 0).values)\n",
    "    train_val_ratio = 0.7\n",
    "    train_dataset, val_dataset = random_split(dataset, \\\n",
    "        [int(dataset.len*train_val_ratio), dataset.len-int(dataset.len*train_val_ratio)], \\\n",
    "        generator=torch.Generator().manual_seed(42))\n",
    "    del dataset\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef61ad83",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fc9352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "171df096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import baseline\n",
    "\n",
    "# Modify device to always use CPU (since CUDA is not available on Mac)\n",
    "device = 'cpu'  # Force usage of CPU on macOS\n",
    "\n",
    "export_onnx = True\n",
    "net = baseline.Net().to(device)  # Ensure the model is sent to the correct device (CPU)\n",
    "net.apply(init_weights)\n",
    "\n",
    "if export_onnx:\n",
    "    import torch.onnx\n",
    "    x = torch.randn([1,1,64,60]).to(device)  # Create dummy input and ensure it's on CPU\n",
    "    torch.onnx.export(net,               # model being run\n",
    "                      x,                 # model input (or a tuple for multiple inputs)\n",
    "                      \"../cnn_baseline.onnx\",  # where to save the model (can be a file or file-like object)\n",
    "                      export_params=False,    # store the trained parameter weights inside the model file\n",
    "                      opset_version=10,       # the ONNX version to export the model to\n",
    "                      do_constant_folding=False, # whether to execute constant folding for optimization\n",
    "                      input_names = ['input_images'],  # the model's input names\n",
    "                      output_names = ['output_prob'],  # the model's output names\n",
    "                      dynamic_axes={'input_images' : {0 : 'batch_size'},  # variable length axes\n",
    "                                    'output_prob' : {0 : 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4905ac02",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fab1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the number of images matches the number of labels\n",
    "num_labels = len(label_df)\n",
    "images = images[:num_labels]  # Truncate images to match the number of labels\n",
    "\n",
    "# Now proceed with dataset creation and splitting\n",
    "if not use_ramdon_split:\n",
    "    train_val_ratio = 0.7\n",
    "    split_idx = int(num_labels * train_val_ratio)\n",
    "    \n",
    "    # Use the truncated images and labels\n",
    "    train_dataset = MyDataset(images[:split_idx], (label_df.Ret_20d > 0).values[:split_idx])\n",
    "    val_dataset = MyDataset(images[split_idx:], (label_df.Ret_20d > 0).values[split_idx:])\n",
    "else:\n",
    "    dataset = MyDataset(images, (label_df.Ret_20d > 0).values)\n",
    "    train_val_ratio = 0.7\n",
    "    train_dataset, val_dataset = random_split(dataset, \\\n",
    "        [int(dataset.len*train_val_ratio), dataset.len-int(dataset.len*train_val_ratio)], \\\n",
    "        generator=torch.Generator().manual_seed(42))\n",
    "    del dataset\n",
    "\n",
    "# DataLoader with drop_last to avoid incomplete batches\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, pin_memory=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a0d421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.0.weight : torch.Size([64, 1, 5, 3])\n",
      "layer1.0.bias : torch.Size([64])\n",
      "layer1.1.weight : torch.Size([64])\n",
      "layer1.1.bias : torch.Size([64])\n",
      "layer2.0.weight : torch.Size([128, 64, 5, 3])\n",
      "layer2.0.bias : torch.Size([128])\n",
      "layer2.1.weight : torch.Size([128])\n",
      "layer2.1.bias : torch.Size([128])\n",
      "layer3.0.weight : torch.Size([256, 128, 5, 3])\n",
      "layer3.0.bias : torch.Size([256])\n",
      "layer3.1.weight : torch.Size([256])\n",
      "layer3.1.bias : torch.Size([256])\n",
      "fc1.1.weight : torch.Size([2, 46080])\n",
      "fc1.1.bias : torch.Size([2])\n",
      "total_parameters : 708866\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for name, parameters in net.named_parameters():\n",
    "    print(name, ':', parameters.size())\n",
    "    count += parameters.numel()\n",
    "print('total_parameters : {}'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a91740b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 2740\n",
      "Total labels: 2740\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total images: {len(images)}\")\n",
    "print(f\"Total labels: {len(label_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a610446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.0.weight : torch.Size([64, 1, 5, 3])\n",
      "layer1.0.bias : torch.Size([64])\n",
      "layer1.1.weight : torch.Size([64])\n",
      "layer1.1.bias : torch.Size([64])\n",
      "layer2.0.weight : torch.Size([128, 64, 5, 3])\n",
      "layer2.0.bias : torch.Size([128])\n",
      "layer2.1.weight : torch.Size([128])\n",
      "layer2.1.bias : torch.Size([128])\n",
      "layer3.0.weight : torch.Size([256, 128, 5, 3])\n",
      "layer3.0.bias : torch.Size([256])\n",
      "layer3.1.weight : torch.Size([256])\n",
      "layer3.1.bias : torch.Size([256])\n",
      "fc1.1.weight : torch.Size([2, 46080])\n",
      "fc1.1.bias : torch.Size([2])\n",
      "total_parameters : 708866\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for name, parameters in net.named_parameters():\n",
    "    print(name, ':', parameters.size())\n",
    "    count += parameters.numel()\n",
    "print('total_parameters : {}'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3f74b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import baseline\n",
    "from thop import profile as thop_profile\n",
    "\n",
    "\n",
    "net = baseline.Net().to(device)  # Make sure you're using the Net class from baseline_2\n",
    "input_data = next(iter(train_dataloader))[0].to(device)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "input_data = input_data.view(-1, 1, 64, 96)\n",
    "input_data = F.interpolate(input_data, size=(64, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfed056e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "FLOPs = 0.28296576G\n",
      "Params = 0.708866M\n"
     ]
    }
   ],
   "source": [
    "# Calculate FLOPs and parameters using the baseline_2 model\n",
    "flops, params = thop_profile(net, inputs=(input_data,))\n",
    "\n",
    "# Print FLOPs and parameters in a more readable format\n",
    "print('FLOPs = ' + str(flops/1000**3) + 'G')\n",
    "print('Params = ' + str(params/1000**2) + 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7a9788f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  model_inference        12.56%       3.261ms       100.00%      25.952ms      25.952ms             1  \n",
      "                    aten::reshape         0.03%       6.876us         0.10%      25.792us      12.896us             2  \n",
      "                       aten::view         0.07%      18.916us         0.07%      18.916us       9.458us             2  \n",
      "                     aten::conv2d         0.09%      22.210us        37.77%       9.803ms       3.268ms             3  \n",
      "                aten::convolution         5.94%       1.542ms        37.69%       9.781ms       3.260ms             3  \n",
      "               aten::_convolution         2.14%     554.166us        31.75%       8.239ms       2.746ms             3  \n",
      "        aten::slow_conv_dilated2d        23.22%       6.027ms        29.61%       7.685ms       2.562ms             3  \n",
      "                      aten::empty         0.18%      46.330us         0.18%      46.330us       1.495us            31  \n",
      "                    aten::resize_         0.02%       4.541us         0.02%       4.541us       1.514us             3  \n",
      "                     aten::select         2.84%     736.791us         3.71%     963.864us       1.069us           902  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 25.952ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helina_cheng/anaconda3/lib/python3.11/site-packages/torch/autograd/profiler.py:257: UserWarning: CUDA is not available, disabling CUDA profiling\n",
      "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "inputs = next(iter(train_dataloader))[0].to(device)\n",
    "inputs = inputs.view(-1, 1, 64, 96)\n",
    "inputs = F.interpolate(inputs, size=(64, 60))\n",
    "\n",
    "with profile(activities=[\n",
    "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        net(inputs)\n",
    "\n",
    "prof.export_chrome_trace(\"../trace.json\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8a6fad",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9853fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, net, loss_fn, optimizer):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    current = 0\n",
    "    net.train()\n",
    "    \n",
    "    with tqdm(dataloader) as t:\n",
    "        for batch, (X, y) in enumerate(t):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # Reshape and interpolate the input data\n",
    "            X = X.view(-1, 1, 64, 96)  # Adjust the shape but keep the batch size (X.size(0))\n",
    "            X = F.interpolate(X, size=(64, 60))  # Interpolate to 64x60\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = net(X)\n",
    "            loss = loss_fn(y_pred, y.long())\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update running loss\n",
    "            running_loss = (len(X) * loss.item() + running_loss * current) / (len(X) + current)\n",
    "            current += len(X)\n",
    "            t.set_postfix({'running_loss': running_loss})\n",
    "    \n",
    "    return running_loss\n",
    "\n",
    "def val_loop(dataloader, net, loss_fn):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    current = 0\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(dataloader) as t:\n",
    "            for batch, (X, y) in enumerate(t):\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                # Reshape and interpolate the input data\n",
    "                X = X.view(-1, 1, 64, 96)  # Adjust the shape but keep the batch size (X.size(0))\n",
    "                X = F.interpolate(X, size=(64, 60))  # Interpolate to 64x60\n",
    "                \n",
    "                y_pred = net(X)\n",
    "                loss = loss_fn(y_pred, y.long())\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                running_loss = (len(X) * running_loss + loss.item() * current) / (len(X) + current)\n",
    "                current += len(X)\n",
    "            \n",
    "    return running_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0af0f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu and use_dataparallel and 'DataParallel' not in str(type(net)):\n",
    "    net = net.to(device)\n",
    "    net = nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df6b2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-5)\n",
    "\n",
    "start_epoch = 0\n",
    "min_val_loss = 1e9\n",
    "last_min_ind = -1\n",
    "early_stopping_epoch = 5\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f17c89e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (128).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loop(train_dataloader, net, loss_fn, optimizer)\n\u001b[1;32m      8\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m96\u001b[39m)\n\u001b[1;32m      9\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(train_loss, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m60\u001b[39m))\n",
      "Cell \u001b[0;32mIn[51], line 18\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, net, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     17\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m net(X)\n\u001b[0;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1189\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1190\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (128)."
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now().strftime('%Y%m%d_%H:%M:%S')\n",
    "os.mkdir('../pt'+os.sep+start_time)\n",
    "epochs = 100\n",
    "for t in range(start_epoch, epochs):\n",
    "    print(f\"Epoch {t}\\n-------------------------------\")\n",
    "    time.sleep(0.2)\n",
    "    train_loss = train_loop(train_dataloader, net, loss_fn, optimizer)\n",
    "    train_loss = train_loss.view(-1, 1, 64, 96)\n",
    "    train_loss = F.interpolate(train_loss, size=(64, 60))\n",
    "    \n",
    "    val_loss = val_loop(val_dataloader, net, loss_fn)\n",
    "    tb.add_histogram(\"train_loss\", train_loss, t)\n",
    "    torch.save(net, '../pt'+os.sep+start_time+os.sep+'baseline_epoch_{}_train_{:5f}_val_{:5f}.pt'.format(t, train_loss, val_loss)) \n",
    "    if val_loss < min_val_loss:\n",
    "        last_min_ind = t\n",
    "        min_val_loss = val_loss\n",
    "    elif t - last_min_ind >= early_stopping_epoch:\n",
    "        break\n",
    "\n",
    "print('Done!')\n",
    "print('Best epoch: {}, val_loss: {}'.format(last_min_ind, min_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5a59d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
